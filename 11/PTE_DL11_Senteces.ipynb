{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karsarobert/Deep-Learning-2023/blob/main/11/PTE_DL11_Senteces.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opposed-female"
      },
      "source": [
        "# Deep Learning gyakorlat\n",
        "\n",
        "\n",
        "## 11. gyakorlat: word2vec\n",
        "### 2022. november 23."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX9Yx50TUies"
      },
      "source": [
        "#Szöveg előkészítése a TensorFlow modellekkel való használatra\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l09c02_nlp_padding.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l09c02_nlp_padding.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_MCdtjT-bly"
      },
      "source": [
        "A gépi tanulási modellben felhasználható szöveg előkészítésének magas szintű lépései a következők:\n",
        "\n",
        "1.   Tokenizáljuk a szavakat, hogy számértékeket kapjunk számukra.\n",
        "2.   A mondatok számsorozatainak létrehozása\n",
        "3.   Állítsuk be a sorozatok hosszát úgy, hogy mindegyik azonos legyen.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJsd8KslUn7j"
      },
      "source": [
        "## Importáljuk a szükséges osztályokat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZxQf11OUtQI"
      },
      "outputs": [],
      "source": [
        "# Import Tokenizer and pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MeEgRq4WX0v"
      },
      "source": [
        "## Írjunk néhány mondatot\n",
        "\n",
        "Lehet változtatni is a szövegen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwM7IP2lTr7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c514196-c9e2-4486-ab5b-9405a1df4c43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['My favorite food is ice cream', 'do you like ice cream too?', 'My dog likes ice cream!', 'your favorite flavor of icecream is chocolate', \"chocolate isn't good for dogs\", 'your dog, your cat, and your parrot prefer broccoli']\n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "    'My favorite food is ice cream',\n",
        "    'do you like ice cream too?',\n",
        "    'My dog likes ice cream!',\n",
        "    \"your favorite flavor of icecream is chocolate\",\n",
        "    \"chocolate isn't good for dogs\",\n",
        "    \"your dog, your cat, and your parrot prefer broccoli\"\n",
        "]\n",
        "print(sentences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaRa9opNWmA7"
      },
      "source": [
        "## Hozzuk létre a tokenizálót és definiáljunk egy szókészleten kívüli tokent\n",
        "A Tokenizer létrehozásakor megadhatja a szótárban lévő szavak maximális számát. Megadhat egy tokent is, amely a szókészleten kívüli (OOV), azaz a szótárban nem szereplő szavakat jelöli. Ez az OOV token akkor kerül felhasználásra, amikor olyan mondatokhoz készít szekvenciákat, amelyek olyan szavakat tartalmaznak, amelyek nem szerepelnek a szóindexben."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7wuOJaBWiHZ"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7nILrhKXPge"
      },
      "source": [
        "## A szavak tokenizálása"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXooiuwrXROU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dca44849-0e45-4299-e397-1e2dec9ab2cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'your': 2, 'ice': 3, 'cream': 4, 'my': 5, 'favorite': 6, 'is': 7, 'dog': 8, 'chocolate': 9, 'food': 10, 'do': 11, 'you': 12, 'like': 13, 'too': 14, 'likes': 15, 'flavor': 16, 'of': 17, 'icecream': 18, \"isn't\": 19, 'good': 20, 'for': 21, 'dogs': 22, 'cat': 23, 'and': 24, 'parrot': 25, 'prefer': 26, 'broccoli': 27}\n"
          ]
        }
      ],
      "source": [
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U-oe201Xm7T"
      },
      "source": [
        "## Alakítsuk a mondatokat szekvenciákká\n",
        "\n",
        "Mostantól minden szónak egyedi száma van a szóindexben.  A mondatban lévő szavak azonban meghatározott sorrendben vannak. Nem lehet csak úgy véletlenszerűen összekeverni a szavakat, és az eredmény egy mondat lesz.\n",
        "\n",
        "A következő lépés tehát a szöveg olyan módon történő ábrázolásához, amelyet a gépi tanulási programok értelmesen használhatnak, az, hogy a szövegben szereplő mondatokat ábrázoló számsorozatokat hozzunk létre.\n",
        "\n",
        "Minden egyes mondatot olyan szekvenciává alakítunk át, amelyben minden egyes szót a szóindexben szereplő számmal helyettesítünk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70l5x1XRXoV4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc0b68c-b26f-491b-fd77-cc6081e9dd13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5, 6, 10, 7, 3, 4], [11, 12, 13, 3, 4, 14], [5, 8, 15, 3, 4], [2, 6, 16, 17, 18, 7, 9], [9, 19, 20, 21, 22], [2, 8, 2, 23, 24, 2, 25, 26, 27]]\n"
          ]
        }
      ],
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print (sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcFghvQ34cZK"
      },
      "source": [
        "## A szekvenciák mindegyike legyen azonos hosszúságú.\n",
        "\n",
        "Később, amikor a szekvenciákat egy neurális hálózatba tápláljuk a modell betanításához, a szekvenciáknak egyforma méretűeknek kell lenniük. Jelenleg a szekvenciák különböző hosszúságúak, ezért a következő lépés az, hogy a szekvenciák azonos méretűek legyenek, vagy nullákkal való kitöltéssel és/vagy csonkítással.\n",
        "\n",
        "Az f.keras.preprocessing.sequence.pad_sequences segítségével nullákat adhatunk hozzá a szekvenciákhoz, hogy mindegyik azonos hosszúságú legyen. Alapértelmezés szerint a kitöltés a szekvenciák elejére kerül, de megadhatja, hogy a szekvenciák végére kerüljön a kitöltés.\n",
        "\n",
        "Opcionálisan megadhatja a maximális hosszúságot, ameddig a szekvenciákat fel kell tölteni. A megadott maximális hossznál hosszabb szekvenciákat a rendszer le fogja vágni. Alapértelmezés szerint a szekvenciákat a szekvencia elejétől kezdve vágja le a rendszer, de megadhatja, hogy a szekvencia végétől kezdve vágja le.\n",
        "\n",
        "Ha nem adja meg a maximális hosszúságot, akkor a szekvenciák a leghosszabb mondat hosszának megfelelően lesznek kitöltve.\n",
        "\n",
        "A szekvenciák kitöltésére és csonkítására vonatkozó összes lehetőséget lásd a https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences oldalon.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0m_nqHg4gqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55b12482-1e3e-4cbe-e817-d70f012079ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Index =  {'<OOV>': 1, 'your': 2, 'ice': 3, 'cream': 4, 'my': 5, 'favorite': 6, 'is': 7, 'dog': 8, 'chocolate': 9, 'food': 10, 'do': 11, 'you': 12, 'like': 13, 'too': 14, 'likes': 15, 'flavor': 16, 'of': 17, 'icecream': 18, \"isn't\": 19, 'good': 20, 'for': 21, 'dogs': 22, 'cat': 23, 'and': 24, 'parrot': 25, 'prefer': 26, 'broccoli': 27}\n",
            "\n",
            "Sequences =  [[5, 6, 10, 7, 3, 4], [11, 12, 13, 3, 4, 14], [5, 8, 15, 3, 4], [2, 6, 16, 17, 18, 7, 9], [9, 19, 20, 21, 22], [2, 8, 2, 23, 24, 2, 25, 26, 27]]\n",
            "\n",
            "Padded Sequences:\n",
            "[[ 0  0  0  5  6 10  7  3  4]\n",
            " [ 0  0  0 11 12 13  3  4 14]\n",
            " [ 0  0  0  0  5  8 15  3  4]\n",
            " [ 0  0  2  6 16 17 18  7  9]\n",
            " [ 0  0  0  0  9 19 20 21 22]\n",
            " [ 2  8  2 23 24  2 25 26 27]]\n"
          ]
        }
      ],
      "source": [
        "padded = pad_sequences(sequences)\n",
        "print(\"\\nWord Index = \" , word_index)\n",
        "print(\"\\nSequences = \" , sequences)\n",
        "print(\"\\nPadded Sequences:\")\n",
        "print(padded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzbGtYWQ6Ofo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d687bb-9061-4d74-e30e-90524df1253b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  0  0  0  0  0  0  5  6 10  7  3  4]\n",
            " [ 0  0  0  0  0  0  0  0  0 11 12 13  3  4 14]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  5  8 15  3  4]\n",
            " [ 0  0  0  0  0  0  0  0  2  6 16 17 18  7  9]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  9 19 20 21 22]\n",
            " [ 0  0  0  0  0  0  2  8  2 23 24  2 25 26 27]]\n"
          ]
        }
      ],
      "source": [
        "# Specify a max length for the padded sequences\n",
        "padded = pad_sequences(sequences, maxlen=15)\n",
        "print(padded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzkbHi0B64w8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e290ea6f-228b-4329-e26d-b91f9ebb03e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 5  6 10  7  3  4  0  0  0  0  0  0  0  0  0]\n",
            " [11 12 13  3  4 14  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  8 15  3  4  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  6 16 17 18  7  9  0  0  0  0  0  0  0  0]\n",
            " [ 9 19 20 21 22  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  8  2 23 24  2 25 26 27  0  0  0  0  0  0]]\n"
          ]
        }
      ],
      "source": [
        "# Put the padding at the end of the sequences\n",
        "padded = pad_sequences(sequences, maxlen=15, padding=\"post\")\n",
        "print(padded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLHheI477okX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25eacffa-b069-4dae-b1ed-fb7f173e98c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 7  3  4]\n",
            " [ 3  4 14]\n",
            " [15  3  4]\n",
            " [18  7  9]\n",
            " [20 21 22]\n",
            " [25 26 27]]\n"
          ]
        }
      ],
      "source": [
        "# Limit the length of the sequences, you will see some sequences get truncated\n",
        "padded = pad_sequences(sequences, maxlen=3)\n",
        "print(padded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnRKDsR197-J"
      },
      "source": [
        "## Mi történik, ha néhány mondat olyan szavakat tartalmaz, amelyek nem szerepelnek a szóindexben?\n",
        "\n",
        "Itt használjuk a \"szókészleten kívüli\" tokent. Próbáljon meg szekvenciákat generálni néhány olyan mondathoz, amelyekben olyan szavak vannak, amelyek nem szerepelnek a szóindexben."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqodOpn64c2U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e008536-3610-4cb3-d925-1fc97280e117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"my best friend's favorite ice cream flavor is strawberry\", \"my dog's best friend is a manatee\"]\n",
            "<OOV> has the number 1 in the word index.\n",
            "\n",
            "Test Sequence =  [[5, 1, 1, 6, 3, 4, 16, 7, 1], [5, 1, 1, 1, 7, 1, 1]]\n",
            "\n",
            "Padded Test Sequence: \n",
            "[[ 0  5  1  1  6  3  4 16  7  1]\n",
            " [ 0  0  0  5  1  1  1  7  1  1]]\n"
          ]
        }
      ],
      "source": [
        "# Try turning sentences that contain words that\n",
        "# aren't in the word index into sequences.\n",
        "# Add your own sentences to the test_data\n",
        "test_data = [\n",
        "    \"my best friend's favorite ice cream flavor is strawberry\",\n",
        "    \"my dog's best friend is a manatee\"\n",
        "]\n",
        "print (test_data)\n",
        "\n",
        "# Remind ourselves which number corresponds to the\n",
        "# out of vocabulary token in the word index\n",
        "print(\"<OOV> has the number\", word_index['<OOV>'], \"in the word index.\")\n",
        "\n",
        "# Convert the test sentences to sequences\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(\"\\nTest Sequence = \", test_seq)\n",
        "\n",
        "# Pad the new sequences\n",
        "padded = pad_sequences(test_seq, maxlen=10)\n",
        "print(\"\\nPadded Test Sequence: \")\n",
        "\n",
        "# Notice that \"1\" appears in the sequence wherever there's a word\n",
        "# that's not in the word index\n",
        "print(padded)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}